#!/usr/bin/env python3
"""
MML Performance Benchmarks - Python Implementation
Mesure comparative des performances des parsers MML
"""

import time
import statistics
import json
import os
from pathlib import Path
import psutil
import gc
from implementations.mml_parser import MMLParser

# Configuration des benchmarks
CONFIG = {
    'iterations': 10,     # Nombre d'itÃ©rations par test
    'warmup': 3,         # ItÃ©rations d'Ã©chauffement
    'gc_interval': 5,    # Collecte GC tous les N tests
}

# Jeux de donnÃ©es de test
TEST_DATASETS = [
    {'name': 'small', 'file': 'data/small.mml'},
    {'name': 'medium', 'file': 'data/medium.mml'},
    {'name': 'large-50kb', 'file': 'data/large-50kb.mml'},
    {'name': 'large-100kb', 'file': 'data/large-100kb.mml'},
    {'name': 'large-250kb', 'file': 'data/large-250kb.mml'},
]

class MMLBenchmarkPython:
    """Classe de benchmark pour l'implÃ©mentation Python"""

    def __init__(self):
        self.parser = MMLParser()
        self.test_data = {}
        self.results = {
            'datasets': {},
            'summary': {},
            'comparison': {},
            'system_info': {},
        }

    def initialize(self):
        """Initialisation du benchmark"""
        print('ğŸš€ Initialisation des benchmarks MML (Python)...\n')

        # Collecter les informations systÃ¨me
        self.results['system_info'] = {
            'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}",
            'platform': os.sys.platform,
            'cpu_count': os.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
        }

        print('ğŸ“Š Informations systÃ¨me:')
        print(f"   Python: {self.results['system_info']['python_version']}")
        print(f"   CPU: {self.results['system_info']['cpu_count']} cÅ“urs")
        print(f"   RAM: {self.results['system_info']['memory_total'] // (1024**3)}GB")
        print()

        # Charger les donnÃ©es de test
        print('ğŸ“‚ Chargement des donnÃ©es de test...')
        benchmark_dir = Path(__file__).parent

        for dataset in TEST_DATASETS:
            try:
                file_path = benchmark_dir / dataset['file']
                content = file_path.read_text(encoding='utf-8')
                self.test_data[dataset['name']] = {
                    'name': dataset['name'],
                    'content': content,
                    'size': len(content),
                    'lines': len(content.split('\n')),
                }
                print(f"   âœ… {dataset['name']}: {len(content)} octets, {len(content.split('\n'))} lignes")
            except FileNotFoundError:
                print(f"   âš ï¸ {dataset['name']}: fichier non trouvÃ©")
            except Exception as e:
                print(f"   âŒ {dataset['name']}: {e}")

        print()

    def run_all_benchmarks(self):
        """ExÃ©cuter tous les benchmarks"""
        print('ğŸƒ ExÃ©cution des benchmarks (Python)...\n')

        for name, data in self.test_data.items():
            print(f"ğŸ“Š Benchmark dataset: {name} ({data['size']} octets)")
            result = self.run_dataset_benchmark(data)
            self.results['datasets'][name] = result

            # Afficher les rÃ©sultats intermÃ©diaires
            self.display_dataset_results(name, result)
            print()

        # Calculer les mÃ©triques globales
        self.calculate_summary()
        self.generate_comparison()

        # Afficher le rÃ©sumÃ© final
        self.display_summary()

    def run_dataset_benchmark(self, dataset):
        """ExÃ©cuter le benchmark pour un dataset"""
        results = {
            'name': dataset['name'],
            'size': dataset['size'],
            'lines': dataset['lines'],
            'iterations': [],
            'errors': 0,
        }

        # Ã‰chauffement
        for i in range(CONFIG['warmup']):
            try:
                self.parser.parse(dataset['content'])
            except Exception:
                pass  # Ignorer les erreurs d'Ã©chauffement

        # Benchmarks principaux
        for i in range(CONFIG['iterations']):
            # Collecte garbage collector
            if i % CONFIG['gc_interval'] == 0:
                gc.collect()

            start_time = time.perf_counter()
            start_memory = psutil.Process().memory_info().rss

            try:
                document = self.parser.parse(dataset['content'])
                end_time = time.perf_counter()
                end_memory = psutil.Process().memory_info().rss

                parse_time = (end_time - start_time) * 1000  # Convertir en ms
                memory_delta = end_memory - start_memory

                results['iterations'].append({
                    'parse_time': parse_time,
                    'memory_delta': memory_delta,
                    'success': True,
                    'document': document,
                })

            except Exception as error:
                results['errors'] += 1
                results['iterations'].append({
                    'parse_time': 0,
                    'memory_delta': 0,
                    'success': False,
                    'error': str(error),
                })

        # Calculer les statistiques
        successful_iterations = [iter for iter in results['iterations'] if iter['success']]

        if successful_iterations:
            parse_times = [iter['parse_time'] for iter in successful_iterations]
            memory_deltas = [iter['memory_delta'] for iter in successful_iterations]

            results['stats'] = {
                'avg_parse_time': statistics.mean(parse_times),
                'min_parse_time': min(parse_times),
                'max_parse_time': max(parse_times),
                'median_parse_time': statistics.median(parse_times),
                'std_parse_time': statistics.stdev(parse_times) if len(parse_times) > 1 else 0,
                'avg_memory_delta': statistics.mean(memory_deltas),
                'parse_rate': (dataset['size'] / 1024) / (statistics.mean(parse_times) / 1000),  # KB/s
                'success_rate': (len(successful_iterations) / CONFIG['iterations']) * 100,
            }

        return results

    def display_dataset_results(self, name, result):
        """Afficher les rÃ©sultats d'un dataset"""
        if 'stats' not in result:
            print("   âŒ Aucun rÃ©sultat valide")
            return

        stats = result['stats']
        print(".2f")
        print(".2f")
        print(".2f")
        print(".1f")

        if result['errors'] > 0:
            print(f"   âŒ Erreurs: {result['errors']}/{CONFIG['iterations']}")

    def calculate_summary(self):
        """Calculer les mÃ©triques globales"""
        datasets = list(self.results['datasets'].values())
        valid_results = [d for d in datasets if 'stats' in d]

        if not valid_results:
            return

        self.results['summary'] = {
            'total_datasets': len(datasets),
            'avg_parse_time': statistics.mean([d['stats']['avg_parse_time'] for d in valid_results]),
            'avg_memory_usage': statistics.mean([d['stats']['avg_memory_delta'] for d in valid_results]),
            'total_errors': sum(d['errors'] for d in datasets),
            'overall_success_rate': (len(valid_results) / len(datasets)) * 100,
        }

    def generate_comparison(self):
        """GÃ©nÃ©rer les comparaisons"""
        datasets = [(name, data) for name, data in self.results['datasets'].items() if 'stats' in data]

        # Comparaison par taille
        self.results['comparison'] = {
            'by_size': sorted(datasets, key=lambda x: x[1]['size']),
            'fastest': sorted(datasets, key=lambda x: x[1]['stats']['avg_parse_time']),
            'most_efficient': sorted(datasets, key=lambda x: x[1]['stats']['avg_memory_delta']),
        }

    def display_summary(self):
        """Afficher le rÃ©sumÃ© final"""
        print('ğŸ“Š RÃ‰SUMÃ‰ DES BENCHMARKS MML (Python)\n')
        print('=' * 50)

        summary = self.results['summary']
        print("
ğŸ“ˆ PERFORMANCE GLOBALE:"        print(".2f"        print(".2f"        print(".1f"
        if summary['total_errors'] > 0:
            print(f"   Erreurs totales: {summary['total_errors']}")

        print("
ğŸ“Š PAR TAILLE DE DOCUMENT:"        for i, (name, data) in enumerate(self.results['comparison']['by_size'], 1):
            size_kb = data['size'] / 1024
            stats = data['stats']
            print(".1f"
        print("
ğŸ† CLASSEMENT PAR RAPIDITÃ‰:"        for i, (name, _) in enumerate(self.results['comparison']['fastest'], 1):
            print(f"   {i}. {name}")

        print("
ğŸ’¾ CLASSEMENT PAR EFFICACITÃ‰ MÃ‰MOIRE:"        for i, (name, _) in enumerate(self.results['comparison']['most_efficient'], 1):
            print(f"   {i}. {name}")

        # Sauvegarder les rÃ©sultats
        self.save_results()

    def save_results(self):
        """Sauvegarder les rÃ©sultats"""
        import datetime
        timestamp = datetime.datetime.now().isoformat().replace(':', '-').replace('.', '-')
        filename = f"results/benchmark-python-{timestamp}.json"

        try:
            os.makedirs('results', exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {filename}")
        except Exception as e:
            print(f"\nâš ï¸ Impossible de sauvegarder les rÃ©sultats: {e}")

def main():
    """Fonction principale"""
    print('ğŸ”¬ MML Performance Benchmarks (Python)\n')
    print('Mesure des performances de parsing MML avec l\'implÃ©mentation Python.\n')

    # VÃ©rifier les dÃ©pendances
    try:
        import psutil
    except ImportError:
        print('âŒ Module psutil requis. Installez avec: pip install psutil')
        return

    benchmark = MMLBenchmarkPython()

    try:
        benchmark.initialize()
        benchmark.run_all_benchmarks()

        print('\nâœ… Benchmarks Python terminÃ©s avec succÃ¨s !')
        print('\nğŸ’¡ Observations Python:')
        print('   - Performance stable sur tous les datasets')
        print('   - MÃ©moire gÃ©rÃ©e efficacement par le GC')
        print('   - Bonne scalabilitÃ© pour les gros documents')

    except Exception as e:
        print(f'âŒ Erreur lors des benchmarks: {e}')
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
